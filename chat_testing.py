import random
import json
import torch
import os
import time
from flask import Flask, request, jsonify
from model import NeuralNet
from nltk_utils import bag_of_words, tokenize
import google.generativeai as gen_ai
from dotenv import load_dotenv

# Load environment variables

app = Flask(__name__)

load_dotenv()

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Load intents
with open("intents.json", "r", encoding="utf-8") as json_data:
    intents = json.load(json_data)

FILE = "data1.pth"
data = torch.load(FILE, weights_only=True)

input_size = data["input_size"]
hidden_size = data["hidden_size"]
output_size = data["output_size"]
all_words = data["all_words"]
tags = data["tags"]
model_state = data["model_state"]

# Initialize neural network model
model = NeuralNet(input_size, hidden_size, output_size).to(device)
model.load_state_dict(model_state)
model.eval()

# Configure Google Gemini model
GOOGLE_API_KEY = "AIzaSyA73rSuqrn6kp_9M85ICgkSv1UtdmhQO-g"
gen_ai.configure(api_key=GOOGLE_API_KEY)
model_ai = gen_ai.GenerativeModel("gemini-pro")


# def format_response(response_text):
#     # Thay th·∫ø d·∫•u "*" b·∫±ng c√°c th·∫ª HTML ho·∫∑c ƒë·ªãnh d·∫°ng kh√°c
#     formatted_response = (
#         response_text.replace(" * ", "<br> - ")
#         .replace("**", "<br>")
#         .replace(":**", "<br>")
#     )
#     return formatted_response
@app.route("/chat", methods=["POST"])
def chat():
    msg = request.json.get("message")
    response = get_response(msg)  # G·ªçi h√†m x·ª≠ l√Ω c√¢u h·ªèi
    return jsonify({"response": response})


def format_response(response_text):
    # Thay th·∫ø d·∫•u ** ƒë·∫ßu ti√™n b·∫±ng <strong>, gi·ªØ nguy√™n d·∫•u * n·∫øu c√≥
    segments = response_text.split("**")

    formatted_segments = []
    for i, segment in enumerate(segments):
        if i % 2 == 1:  # N·∫øu l√† ƒëo·∫°n gi·ªØa (gi·ªØa c√°c d·∫•u **)
            # Ki·ªÉm tra xem c√≥ d·∫•u * ·ªü ƒë·∫ßu ƒëo·∫°n kh√¥ng
            if segment.startswith(" * "):
                # Thay d·∫•u * ƒë·∫ßu ti√™n b·∫±ng kho·∫£ng tr·∫Øng v√† gi·ªØ nguy√™n ƒëo·∫°n
                formatted_segments.append(segment.replace(" * ", "<br>", 1).strip())
            else:
                formatted_segments.append(f"<strong>{segment.strip()}</strong>")
        else:
            # Ki·ªÉm tra d·∫•u ch·∫•m ·ªü cu·ªëi ƒëo·∫°n
            if segment.strip().endswith("."):
                formatted_segments.append(
                    segment.strip() + "<br>"
                )  # Th√™m <br> n·∫øu c√≥ d·∫•u ch·∫•m ·ªü cu·ªëi
            else:
                formatted_segments.append(segment.strip())
    formatted_response = "".join(formatted_segments) + "<br>"
    formatted_response = formatted_response.replace(
        segments[1], segments[1] + "<br>", 1
    )

    # Thay th·∫ø d·∫•u * b·∫±ng d·∫•u ng·∫Øt d√≤ng
    formatted_response = formatted_response.replace("*", "<br> - ")

    return formatted_response


def get_response_from_gemini(msg):
    try:
        # G·ªçi API Gemini ƒë·ªÉ nh·∫≠n ph·∫£n h·ªìi
        response = model_ai.generate_content(msg)
        return format_response(response.text.strip())
        # return response.text.strip()
    except Exception as e:
        print(f"C√≥ l·ªói khi g·ªçi Gemini: {e}")
        return "Xin l·ªói, t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y ngay b√¢y gi·ªù."


use_ai_bot = False


def get_response(msg):
    global use_ai_bot
    if msg.lower() == "ai bot" or msg.lower() == "ai chat":
        use_ai_bot = True
        return "ü§ñCh·∫ø ƒë·ªô AI Bot ƒë√£ ƒë∆∞·ª£c k√≠ch ho·∫°t. M·ªçi c√¢u h·ªèi ti·∫øp theo s·∫Ω ƒë∆∞·ª£c tr·∫£ l·ªùi b·ªüi m√¥ h√¨nh AI.ü§ñ"

    # Ki·ªÉm tra n·∫øu ng∆∞·ªùi d√πng mu·ªën quay l·∫°i ch·∫ø ƒë·ªô chatbot h·ªó tr·ª£
    if msg.lower() == "support bot" or msg.lower() == "support chat":
        use_ai_bot = False
        return "üß†Ch·∫ø ƒë·ªô Support Bot ƒë√£ ƒë∆∞·ª£c k√≠ch ho·∫°t. M·ªçi c√¢u h·ªèi ti·∫øp theo s·∫Ω ƒë∆∞·ª£c tr·∫£ l·ªùi b·ªüi m√¥ h√¨nh h·ªó tr·ª£."
    # X·ª≠ l√Ω c√¢u h·ªèi b·∫±ng m√¥ h√¨nh

    sentence = tokenize(msg)
    X = bag_of_words(sentence, all_words)
    X = X.reshape(1, X.shape[0])
    X = torch.from_numpy(X).to(device)

    output = model(X)
    _, predicted = torch.max(output, dim=1)

    tag = tags[predicted.item()]

    probs = torch.softmax(output, dim=1)
    prob = probs[0][predicted.item()]
    if use_ai_bot:
        return get_response_from_gemini(msg)
    else:
        if prob.item() > 0.75:
            for intent in intents["intents"]:
                if tag == intent["tag"]:
                    return random.choice(intent["responses"])

        # N·∫øu kh√¥ng c√≥ ph·∫£n h·ªìi n√†o t·ª´ m√¥ h√¨nh, s·ª≠ d·ª•ng Gemini
        return (
            "Xin l·ªói, t√¥i kh√¥ng th·ªÉ tr·∫£ l·ªùi c√¢u h·ªèi n√†y. "
            "<br>"
            "M·ªçi th·∫Øc m·∫Øc li√™n h·ªá trang H·ªó tr·ª£ ho·∫∑c chuy·ªÉn sang chatbot AI."
            "<br>"
            'ƒê·ªÉ chuy·ªÉn ƒë·ªïi gi·ªØa 2 lo·∫°i chatbot AI v√† Support vui l√≤ng nh·∫≠p l·ªánh: AI/Support + Chat ho·∫∑c AI/Support + Bot ".'
        )


if __name__ == "__main__":
    print("Chat! (type 'quit' to exit)")
    while True:
        sentence = input("You: ")
        if sentence == "quit":
            break

        resp = get_response(sentence)
        print(f"Bot: {resp}")
